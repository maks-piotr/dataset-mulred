{
    "id": "kcspfg",
    "title": "Going beyond hashtags: how to ensure AI technology truly benefits everyone",
    "url": "https://www.reddit.com/r/india/comments/kcspfg/going_beyond_hashtags_how_to_ensure_ai_technology/",
    "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/82296146p3561.png?width=1600&amp;format=png&amp;auto=webp&amp;s=b464e4a02952b1f07f1c4715a61f2326c37c6e89\n\n## Tl;dr\n\nNITI Aayog's draft [***Working Document: Enforcement Mechanisms for Responsible #AIforAll***](https://ourgovdotin.files.wordpress.com/2020/11/niti-working-document-enforcement-mechanisms-for-responsible-aiforall.pdf) attempts to build on the the previous *Working Document: Towards Responsible #AIforAll* by providing enforcement mechanisms for the principles laid down in the latter. IFF has submitted its comments on the draft document highlighting the following issues: a need for concrete overarching principles, the extent of the role of the oversight body, robust regulation for the private sector, and risk assessment based restrictions for certain uses of Artificial Intelligence (AI).\n\n## Issues\n\nAs pointed out in [**Part 1**](https://niti.gov.in/sites/default/files/2020-07/Responsible-AI.pdf) of the working document, the use of AI presents manifold risks:\n\n1. AI systems could pick spurious correlations in the underlying data, leading to good accuracy in test datasets but **significant errors in deployment**.\n2. 'Deep Learning' systems remain opaque 'black boxes', showing a high degree of accuracy even though explanations for the same elude technicians, leading to a **lack of trust, accountability, and, ultimately, usage**.\n3. Large scale deployment of AI leads to a large number of high frequency decisions, **amplifying the impact of unfair bias**. This may cause a lack of trust and disruption for social order.\n4. Technological erros may lead to **large scale exclusion of citizens** from services guaranteed by the state.\n5. A lack of consequences **reduces incentive for responsible action**, while **difficulties in the allocation of liability** arise in grievance redressal.\n6. AI systems may use personal data **without the explicit consent** of concerned persons. Advanced technology may also **discern potentially sensitive information** from the outputs of the system.\n7. AI systems are **susceptible to attack** such as manipulation of data being used to train the AI, manipulation of system to respond incorrectly to specific inputs, etc\n8. The rapid rise of AI has led to **automation of a number of routine jobs**, which, without adequate re-skilling and support from the state, may cause social unrest.\n9. **Psychological profiling** enabled by AI and the e**ase of spreading propaganda** through online platforms has potential to cause social disharmony and disrupt democratic process.\n\nPart 1 of the document lays down 7 core principles to mitigate these harms as well as to ensure that a common benchmark for the beneficial use of AI across different sectors is developed. These principles are:\n\n* Principle of Safety and Reliability\n* Principle of Equality\n* Principle of Inclusivity and Non-discrimination\n* Principle of Privacy and security\n* Principle of Transparency\n* Principle of Accountability\n* Principle of protection and reinforcement of positive human values\n\n## What does the draft document say?\n\nThe draft document clarifies the nature and roles of the oversight body for AI technology. It proposes that the oversight body be a highly participatory advisory body, and take on the following roles:\n\n* Manage and update AI principles laid down\n* Research into the various issues related to AI\n* Provide clarity on  design structures, standards, guidelines, etc\n* Promote development and access to AI data and technology tools\n* Help create awareness about responsible AI among various stakeholders\n* Coordinate between different sectoral regulators\n* Represent India in international dialogues on AI\n\nThe draft document aso specifies the need for an AI Ethics Committee in public sector bodies to handle the procurement, development, operations phase of AI systems and to ensure adherence to the Responsible AI principles. Self regulation is recommended for the private sector, with accountability for ethics being assigned to existing leadership.\n\n## Ensuring that AI truly works for All\n\nOur recommendations are based on 4 key issues:\n\n1. **Need for concrete overarching principles:** Part 1 of the working document lists certain principles as principles for responsible AI. However, obscurity still persists with regards to the implication of adoption of these principles in the regulatory framework. Thus, we believe that there is a need to spell out both the detailed meaning of the principles which will work as a foundation to build enforcement mechanisms on as well as the implications of adopting them for regulatory frameworks.\n2. **Greater role for oversight body:** The draft document proposes the Council for Ethics and Technology as an oversight mechanism, which will be a *“highly participatory advisory body”*. We appreciate the formation of such an oversight body, due to the absence of clear laws and legal requirements governing AI, the Council for Ethics and Technology should have more than mere advisory functions and perform a regulatory assurance role, in conjunction with any forthcoming data authority, under the Personal Data Protection Bill, 2019. This should be specifically with the nature of any AI based system that impacts any legal right of a person.\n3. **Robust regulation for the private sector:** The draft document proposes voluntary self-regulation as a starting point for regulating AI in the private sector in India. While self-regulatory efforts are commendable, they should not be made a substitute for laws needed to closely monitor AI. In order to foster a healthy AI ecosystem, soft laws of self-regulation need to be complemented with strict provisions to govern high risk applications of AI.\n4. **Risk assessment based restrictions for different sectors**: All AI is not the same since the term envelopes within its multiple technologies. Thus, on the basis of a sector specific risk-assessment study, proportionate restrictions on the use of AI should be in place until an overarching, regulatory framework has been developed.\n\n## Risk Based Assessment - AI in Facial Recognition Technology\n\n### Procedure\n\nIn our submission, we provided a template risk based assessment for illustrative purposes, which was based on the following procedure:\n\n* **Step 1**: The Council must categorize use cases of AI into the 4 types of Algorithmic Systems  based on the AI Now Institute’s categorization of algorithmic systems used in their 2018 ‘Algorithmic Accountability Policy Toolkit’.\n* **Step 2**: The Council must assess the type of data collected by the AI into anonymous data, personal and sensitive personal data,,\n* **Step 3**: Based on type of data, risk assessment may be done with sensitive personal data having the highest risk assessment and anonymous data the lowest (Scale of 1-3 with 3 being the highest).\n* **Step 4**: Based on the level of risk assessment combined with the type of algorithmic system, the regulation framework may be designed by the Council.\n\n### The dangers of AI in Facial Recognition Technology\n\nHere, we would like to address the use of AI for facial recognition technology (FRT). FRT which collects sensitive personal data for criminal justice purposes should be banned from being developed and deployed in India. Under IFF’s [**Project Panoptic**](https://panoptic.frappe.cloud/), we have been mapping FRT systems across the country which are being developed and deployed without any regulatory framework as well as without any public awareness or transparency pertaining to how they will be used.\n\nBy our estimation, there are currently 19 different FRT projects which are being used by Police and Security/Intelligence agencies at the Central and State level in different stages of development and deployment. This is being done in the absence of a personal data protection law/regime as well as any specific regulation for FRT. The harms of such use are manifold:\n\n* One sort of harm results from the implementation of a faulty FRT system wherein the **technology is inaccurate in identifying and matching faces** from a photo/video to an existing database. Such inaccuracy could lead to a false positive result from the FRT.  This may lead to discrimination and strengthening of existing biases. In the present context, a false positive by a FRT system being used by the police could lead to wrongful arrest and detention of an innocent person.\n* Another type of harm results from the implementation of an accurate FRT system wherein **the technology achieves 100% accuracy in identifying and matching faces** from a photo/video to an existing database. While there have been claims of a fully accurate FRT system, none of these claims have been corroborated by independent review and audit. The National Institute of Standards and Technology (NIST) has extensively tested FRT systems for ‘1:1’ verification and ‘1:many’ identification and how accuracy of these systems vary across demographic groups. These independent studies have concluded that, currently, no FRT system has 100% accuracy.\n* Probe images for FRT systems are often collected by the police t**hrough CCTV cameras installed in public spaces**. Individuals in a CCTV surveilled area may be aware that they are under surveillance but the assumption is that this surveillance is temporary. Use of CCTV in conjunction with FRT would mean their images will be stored for a longer period of time, if not permanently. This data will also be used to extract particular data points such as the facial features and other biometrics (sensitive personal data) which the individual has not consented to sharing when entering a CCTV surveilled zone and these data points can be used to track future movements of the person. Therefore, integration of FRT with a network of CCTV cameras would make **real time mass surveillance extremely easy**.\n\n### Goes against standards set by the Supreme Court\n\nWe would like to emphasise that use of FRT for criminal justice purposes goes beyond the standards laid down by the Hon’ble Supreme Court in *Justice K.S. Puttaswamy vs Union of India* (2017 10 SCC 1). The landmark decision lays down certain thresholds which the State must conform to to justify intrusions by the State into people’s right to privacy protected under Article 21 of the Constitution of India. These thresholds are:\n\n1. **Legality**: *Where the intrusion must take place in the presence of a defined regime of law i.e. there must be an anchoring legislation, with a clear set of provisions.* As we know, there is no anchoring legislation in place to regulate the use of FRT by the police. Additionally, we do not have a data protection regime to oversee the collection, processing and storage of data collected by these systems.\n2. **Necessity**: *Which justifies that the restriction to people’s privacy (in this case data collection and sharing) is needed in a democratic society to fulfill a legitimate state aim.* Use of FRT by the police has been justified based on reasons pertaining to security of the public and the country itself, with proponents saying it will enable automatic identification and verification through criminal databases. This characterisation is based on the faulty assumption that facial recognition technology is accurate, when ongoing research in the field has shown that completely accurate facial recognition technology has not been developed yet. Use of such inaccurate technology, especially for criminal prosecution, could thus result in a false positive.\n3. **Proportionality**: *Where the Government must show among other things that the measure being undertaken has a rational nexus with the objective.* FRT contemplates the collecting of sensitive personal information, intimate information of all individuals in the absence of any reasonable suspicion by collecting images and videos from a scene of crime. This could cast a presumption of criminality on a broad set of people. Collecting sensitive personal information of all individuals who were present at the scene of crime creates a presumption of criminality which is disproportionate to the objective it aims to achieve.\n4. **Procedural safeguards**: *Where there is an appropriate independent institutional mechanism, with in-built procedural safeguards aligned with standards of procedure established by law which are just, fair and reasonable to prevent abuse.* In the absence of any checks and balances, function creep becomes an immediate problem wherein the issue of FRT being used for functions more than its stated purpose becomes a reality. Use of FRT without safeguards could result in illegal state-sponsored mass surveillance which would have a chilling effect on fundamental rights such right to freedom of expression, freedom of movement and freedom of association which are guaranteed in the Constitution.\n\n### Recommended Restriction\n\nUse of this technology has raised concerns not only in India but also abroad with various civil society organisations such as the Electronic Frontier Foundation, the Algorithmic Justice League and Amnesty International calling for ban on the use of this technology. Calls for a total ban have been gaining momentum due to the fear that use of facial recognition by the police and security/intelligence agencies will not only lead to violation of the rights to privacy and freedom of speech and expression but also lead to human rights violations by helping to increase systemic bias against already marginalised communities. The impact on marginalised communities gains special importance for us locally due to the wide inequality and diversity present in our society. Thus, we recommend that the use of FRT be banned.\n\n### Important Documents\n\n1. NITI Aayog's draft *Working Document: Enforcement Mechanisms for Responsible #AIforAll* ([**link**](https://ourgovdotin.files.wordpress.com/2020/11/niti-working-document-enforcement-mechanisms-for-responsible-aiforall.pdf))\n2. NITI Aayog's *Working Document: Towards Responsible #AIforAll* ([**link**](https://niti.gov.in/sites/default/files/2020-07/Responsible-AI.pdf))\n3. IFF's comments on the draft document ([**link**](https://drive.google.com/file/d/1CKvQ3pSSyDjce952u-qedS30ySkze_CS/view?usp=sharing))\n4. IFF's Project Panoptic FRT Tracker ([**link**](https://panoptic.frappe.cloud/))",
    "flair": "Policy/Economy",
    "score": 15,
    "num_comments": 2,
    "created_utc": 1607929129,
    "convurl": "https://b.thumbs.redditmedia.com/JcB_r9rMn9N0zcqy-9jqldGotPvxUflCzZertXCjT8k.jpg",
    "comments": [
        "Man this is really good!",
        "Thank you!!"
    ],
    "cleaned_text": "going beyond hashtags ensure ai technology truly benefits everyone ampxb tldr niti aayogs draft working document enforcement mechanisms responsible aiforallhttpsourgovdotinfileswordpresscomnitiworkingdocumentenforcementmechanismsforresponsibleaiforallpdf attempts build previous working document towards responsible aiforall providing enforcement mechanisms principles laid latter iff submitted comments draft document highlighting following issues need concrete overarching principles extent role oversight body robust regulation private sector risk assessment based restrictions certain uses artificial intelligence ai issues pointed part httpsnitigovinsitesdefaultfilesresponsibleaipdf working document use ai presents manifold risks ai systems could pick spurious correlations underlying data leading good accuracy test datasets significant errors deployment deep learning systems remain opaque black boxes showing high degree accuracy even though explanations elude technicians leading lack trust accountability ultimately usage large scale deployment ai leads large number high frequency decisions amplifying impact unfair bias may cause lack trust disruption social order technological erros may lead large scale exclusion citizens services guaranteed state lack consequences reduces incentive responsible action difficulties allocation liability arise grievance redressal ai systems may use personal data without explicit consent concerned persons advanced technology may also discern potentially sensitive information outputs system ai systems susceptible attack manipulation data used train ai manipulation system respond incorrectly specific inputs etc rapid rise ai led automation number routine jobs without adequate reskilling support state may cause social unrest psychological profiling enabled ai ease spreading propaganda online platforms potential cause social disharmony disrupt democratic process part document lays core principles mitigate harms well ensure common benchmark beneficial use ai across different sectors developed principles principle safety reliability principle equality principle inclusivity nondiscrimination principle privacy security principle transparency principle accountability principle protection reinforcement positive human values draft document say draft document clarifies nature roles oversight body ai technology proposes oversight body highly participatory advisory body take following roles manage update ai principles laid research various issues related ai provide clarity design structures standards guidelines etc promote development access ai data technology tools help create awareness responsible ai among various stakeholders coordinate different sectoral regulators represent india international dialogues ai draft document aso specifies need ai ethics committee public sector bodies handle procurement development operations phase ai systems ensure adherence responsible ai principles self regulation recommended private sector accountability ethics assigned existing leadership ensuring ai truly works recommendations based key issues need concrete overarching principles part working document lists certain principles principles responsible ai however obscurity still persists regards implication adoption principles regulatory framework thus believe need spell detailed meaning principles work foundation build enforcement mechanisms well implications adopting regulatory frameworks greater role oversight body draft document proposes council ethics technology oversight mechanism highly participatory advisory body appreciate formation oversight body due absence clear laws legal requirements governing ai council ethics technology mere advisory functions perform regulatory assurance role conjunction forthcoming data authority personal data protection bill specifically nature ai based system impacts legal right person robust regulation private sector draft document proposes voluntary selfregulation starting point regulating ai private sector india selfregulatory efforts commendable made substitute laws needed closely monitor ai order foster healthy ai ecosystem soft laws selfregulation need complemented strict provisions govern high risk applications ai risk assessment based restrictions different sectors ai since term envelopes within multiple technologies thus basis sector specific riskassessment study proportionate restrictions use ai place overarching regulatory framework developed risk based assessment ai facial recognition technology procedure submission provided template risk based assessment illustrative purposes based following procedure step council must categorize use cases ai types algorithmic systems based ai institutes categorization algorithmic systems used algorithmic accountability policy toolkit step council must assess type data collected ai anonymous data personal sensitive personal data step based type data risk assessment may done sensitive personal data highest risk assessment anonymous data lowest scale highest step based level risk assessment combined type algorithmic system regulation framework may designed council dangers ai facial recognition technology would like address use ai facial recognition technology frt frt collects sensitive personal data criminal justice purposes banned developed deployed india iffs project panoptichttpspanopticfrappecloud mapping frt systems across country developed deployed without regulatory framework well without public awareness transparency pertaining used estimation currently different frt projects used police securityintelligence agencies central state level different stages development deployment done absence personal data protection lawregime well specific regulation frt harms use manifold one sort harm results implementation faulty frt system wherein technology inaccurate identifying matching faces photovideo existing database inaccuracy could lead false positive result frt may lead discrimination strengthening existing biases present context false positive frt system used police could lead wrongful arrest detention innocent person another type harm results implementation accurate frt system wherein technology achieves accuracy identifying matching faces photovideo existing database claims fully accurate frt system none claims corroborated independent review audit national institute standards technology nist extensively tested frt systems verification many identification accuracy systems vary across demographic groups independent studies concluded currently frt system accuracy probe images frt systems often collected police cctv cameras installed public spaces individuals cctv surveilled area may aware surveillance assumption surveillance temporary use cctv conjunction frt would mean images stored longer period time permanently data also used extract particular data points facial features biometrics sensitive personal data individual consented sharing entering cctv surveilled zone data points used track future movements person therefore integration frt network cctv cameras would make real time mass surveillance extremely easy goes standards set supreme court would like emphasise use frt criminal justice purposes goes beyond standards laid honble supreme court justice ks puttaswamy vs union india scc landmark decision lays certain thresholds state must conform justify intrusions state peoples right privacy protected article constitution india thresholds legality intrusion must take place presence defined regime law ie must anchoring legislation clear set provisions know anchoring legislation place regulate use frt police additionally data protection regime oversee collection processing storage data collected systems necessity justifies restriction peoples privacy case data collection sharing needed democratic society fulfill legitimate state aim use frt police justified based reasons pertaining security public country proponents saying enable automatic identification verification criminal databases characterisation based faulty assumption facial recognition technology accurate ongoing research field shown completely accurate facial recognition technology developed yet use inaccurate technology especially criminal prosecution could thus result false positive proportionality government must show among things measure undertaken rational nexus objective frt contemplates collecting sensitive personal information intimate information individuals absence reasonable suspicion collecting images videos scene crime could cast presumption criminality broad set people collecting sensitive personal information individuals present scene crime creates presumption criminality disproportionate objective aims achieve procedural safeguards appropriate independent institutional mechanism inbuilt procedural safeguards aligned standards procedure established law fair reasonable prevent abuse absence checks balances function creep becomes immediate problem wherein issue frt used functions stated purpose becomes reality use frt without safeguards could result illegal statesponsored mass surveillance would chilling effect fundamental rights right freedom expression freedom movement freedom association guaranteed constitution recommended restriction use technology raised concerns india also abroad various civil society organisations electronic frontier foundation algorithmic justice league amnesty international calling ban use technology calls total ban gaining momentum due fear use facial recognition police securityintelligence agencies lead violation rights privacy freedom speech expression also lead human rights violations helping increase systemic bias already marginalised communities impact marginalised communities gains special importance us locally due wide inequality diversity present society thus recommend use frt banned important documents niti aayogs draft working document enforcement mechanisms responsible aiforall linkhttpsourgovdotinfileswordpresscomnitiworkingdocumentenforcementmechanismsforresponsibleaiforallpdf niti aayogs working document towards responsible aiforall linkhttpsnitigovinsitesdefaultfilesresponsibleaipdf iffs comments draft document linkhttpsdrivegooglecomfiledckvqpssydjceuqedsyskzecsviewuspsharing iffs project panoptic frt tracker linkhttpspanopticfrappecloud ",
    "cleaned_title": "going beyond hashtags ensure ai technology truly benefits everyone",
    "cleaned_selftext": "ampxb tldr niti aayogs draft working document enforcement mechanisms responsible aiforallhttpsourgovdotinfileswordpresscomnitiworkingdocumentenforcementmechanismsforresponsibleaiforallpdf attempts build previous working document towards responsible aiforall providing enforcement mechanisms principles laid latter iff submitted comments draft document highlighting following issues need concrete overarching principles extent role oversight body robust regulation private sector risk assessment based restrictions certain uses artificial intelligence ai issues pointed part httpsnitigovinsitesdefaultfilesresponsibleaipdf working document use ai presents manifold risks ai systems could pick spurious correlations underlying data leading good accuracy test datasets significant errors deployment deep learning systems remain opaque black boxes showing high degree accuracy even though explanations elude technicians leading lack trust accountability ultimately usage large scale deployment ai leads large number high frequency decisions amplifying impact unfair bias may cause lack trust disruption social order technological erros may lead large scale exclusion citizens services guaranteed state lack consequences reduces incentive responsible action difficulties allocation liability arise grievance redressal ai systems may use personal data without explicit consent concerned persons advanced technology may also discern potentially sensitive information outputs system ai systems susceptible attack manipulation data used train ai manipulation system respond incorrectly specific inputs etc rapid rise ai led automation number routine jobs without adequate reskilling support state may cause social unrest psychological profiling enabled ai ease spreading propaganda online platforms potential cause social disharmony disrupt democratic process part document lays core principles mitigate harms well ensure common benchmark beneficial use ai across different sectors developed principles principle safety reliability principle equality principle inclusivity nondiscrimination principle privacy security principle transparency principle accountability principle protection reinforcement positive human values draft document say draft document clarifies nature roles oversight body ai technology proposes oversight body highly participatory advisory body take following roles manage update ai principles laid research various issues related ai provide clarity design structures standards guidelines etc promote development access ai data technology tools help create awareness responsible ai among various stakeholders coordinate different sectoral regulators represent india international dialogues ai draft document aso specifies need ai ethics committee public sector bodies handle procurement development operations phase ai systems ensure adherence responsible ai principles self regulation recommended private sector accountability ethics assigned existing leadership ensuring ai truly works recommendations based key issues need concrete overarching principles part working document lists certain principles principles responsible ai however obscurity still persists regards implication adoption principles regulatory framework thus believe need spell detailed meaning principles work foundation build enforcement mechanisms well implications adopting regulatory frameworks greater role oversight body draft document proposes council ethics technology oversight mechanism highly participatory advisory body appreciate formation oversight body due absence clear laws legal requirements governing ai council ethics technology mere advisory functions perform regulatory assurance role conjunction forthcoming data authority personal data protection bill specifically nature ai based system impacts legal right person robust regulation private sector draft document proposes voluntary selfregulation starting point regulating ai private sector india selfregulatory efforts commendable made substitute laws needed closely monitor ai order foster healthy ai ecosystem soft laws selfregulation need complemented strict provisions govern high risk applications ai risk assessment based restrictions different sectors ai since term envelopes within multiple technologies thus basis sector specific riskassessment study proportionate restrictions use ai place overarching regulatory framework developed risk based assessment ai facial recognition technology procedure submission provided template risk based assessment illustrative purposes based following procedure step council must categorize use cases ai types algorithmic systems based ai institutes categorization algorithmic systems used algorithmic accountability policy toolkit step council must assess type data collected ai anonymous data personal sensitive personal data step based type data risk assessment may done sensitive personal data highest risk assessment anonymous data lowest scale highest step based level risk assessment combined type algorithmic system regulation framework may designed council dangers ai facial recognition technology would like address use ai facial recognition technology frt frt collects sensitive personal data criminal justice purposes banned developed deployed india iffs project panoptichttpspanopticfrappecloud mapping frt systems across country developed deployed without regulatory framework well without public awareness transparency pertaining used estimation currently different frt projects used police securityintelligence agencies central state level different stages development deployment done absence personal data protection lawregime well specific regulation frt harms use manifold one sort harm results implementation faulty frt system wherein technology inaccurate identifying matching faces photovideo existing database inaccuracy could lead false positive result frt may lead discrimination strengthening existing biases present context false positive frt system used police could lead wrongful arrest detention innocent person another type harm results implementation accurate frt system wherein technology achieves accuracy identifying matching faces photovideo existing database claims fully accurate frt system none claims corroborated independent review audit national institute standards technology nist extensively tested frt systems verification many identification accuracy systems vary across demographic groups independent studies concluded currently frt system accuracy probe images frt systems often collected police cctv cameras installed public spaces individuals cctv surveilled area may aware surveillance assumption surveillance temporary use cctv conjunction frt would mean images stored longer period time permanently data also used extract particular data points facial features biometrics sensitive personal data individual consented sharing entering cctv surveilled zone data points used track future movements person therefore integration frt network cctv cameras would make real time mass surveillance extremely easy goes standards set supreme court would like emphasise use frt criminal justice purposes goes beyond standards laid honble supreme court justice ks puttaswamy vs union india scc landmark decision lays certain thresholds state must conform justify intrusions state peoples right privacy protected article constitution india thresholds legality intrusion must take place presence defined regime law ie must anchoring legislation clear set provisions know anchoring legislation place regulate use frt police additionally data protection regime oversee collection processing storage data collected systems necessity justifies restriction peoples privacy case data collection sharing needed democratic society fulfill legitimate state aim use frt police justified based reasons pertaining security public country proponents saying enable automatic identification verification criminal databases characterisation based faulty assumption facial recognition technology accurate ongoing research field shown completely accurate facial recognition technology developed yet use inaccurate technology especially criminal prosecution could thus result false positive proportionality government must show among things measure undertaken rational nexus objective frt contemplates collecting sensitive personal information intimate information individuals absence reasonable suspicion collecting images videos scene crime could cast presumption criminality broad set people collecting sensitive personal information individuals present scene crime creates presumption criminality disproportionate objective aims achieve procedural safeguards appropriate independent institutional mechanism inbuilt procedural safeguards aligned standards procedure established law fair reasonable prevent abuse absence checks balances function creep becomes immediate problem wherein issue frt used functions stated purpose becomes reality use frt without safeguards could result illegal statesponsored mass surveillance would chilling effect fundamental rights right freedom expression freedom movement freedom association guaranteed constitution recommended restriction use technology raised concerns india also abroad various civil society organisations electronic frontier foundation algorithmic justice league amnesty international calling ban use technology calls total ban gaining momentum due fear use facial recognition police securityintelligence agencies lead violation rights privacy freedom speech expression also lead human rights violations helping increase systemic bias already marginalised communities impact marginalised communities gains special importance us locally due wide inequality diversity present society thus recommend use frt banned important documents niti aayogs draft working document enforcement mechanisms responsible aiforall linkhttpsourgovdotinfileswordpresscomnitiworkingdocumentenforcementmechanismsforresponsibleaiforallpdf niti aayogs working document towards responsible aiforall linkhttpsnitigovinsitesdefaultfilesresponsibleaipdf iffs comments draft document linkhttpsdrivegooglecomfiledckvqpssydjceuqedsyskzecsviewuspsharing iffs project panoptic frt tracker linkhttpspanopticfrappecloud",
    "cleaned_comments": "man really good thank",
    "light_cleaned_title": "Going beyond hashtags: how to ensure AI technology truly benefits everyone",
    "light_cleaned_selftext": "## Tl;dr NITI Aayog's draft [***Working Document: Enforcement Mechanisms for Responsible #AIforAll***](https://ourgovdotin.files.wordpress.com/2020/11/niti-working-document-enforcement-mechanisms-for-responsible-aiforall.pdf) attempts to build on the the previous *Working Document: Towards Responsible #AIforAll* by providing enforcement mechanisms for the principles laid down in the latter. IFF has submitted its comments on the draft document highlighting the following issues: a need for concrete overarching principles, the extent of the role of the oversight body, robust regulation for the private sector, and risk assessment based restrictions for certain uses of Artificial Intelligence (AI). ## Issues As pointed out in [**Part 1**](https://niti.gov.in/sites/default/files/2020-07/Responsible-AI.pdf) of the working document, the use of AI presents manifold risks: 1. AI systems could pick spurious correlations in the underlying data, leading to good accuracy in test datasets but **significant errors in deployment**. 2. 'Deep Learning' systems remain opaque 'black boxes', showing a high degree of accuracy even though explanations for the same elude technicians, leading to a **lack of trust, accountability, and, ultimately, usage**. 3. Large scale deployment of AI leads to a large number of high frequency decisions, **amplifying the impact of unfair bias**. This may cause a lack of trust and disruption for social order. 4. Technological erros may lead to **large scale exclusion of citizens** from services guaranteed by the state. 5. A lack of consequences **reduces incentive for responsible action**, while **difficulties in the allocation of liability** arise in grievance redressal. 6. AI systems may use personal data **without the explicit consent** of concerned persons. Advanced technology may also **discern potentially sensitive information** from the outputs of the system. 7. AI systems are **susceptible to attack** such as manipulation of data being used to train the AI, manipulation of system to respond incorrectly to specific inputs, etc 8. The rapid rise of AI has led to **automation of a number of routine jobs**, which, without adequate re-skilling and support from the state, may cause social unrest. 9. **Psychological profiling** enabled by AI and the e**ase of spreading propaganda** through online platforms has potential to cause social disharmony and disrupt democratic process. Part 1 of the document lays down 7 core principles to mitigate these harms as well as to ensure that a common benchmark for the beneficial use of AI across different sectors is developed. These principles are: * Principle of Safety and Reliability * Principle of Equality * Principle of Inclusivity and Non-discrimination * Principle of Privacy and security * Principle of Transparency * Principle of Accountability * Principle of protection and reinforcement of positive human values ## What does the draft document say? The draft document clarifies the nature and roles of the oversight body for AI technology. It proposes that the oversight body be a highly participatory advisory body, and take on the following roles: * Manage and update AI principles laid down * Research into the various issues related to AI * Provide clarity on design structures, standards, guidelines, etc * Promote development and access to AI data and technology tools * Help create awareness about responsible AI among various stakeholders * Coordinate between different sectoral regulators * Represent India in international dialogues on AI The draft document aso specifies the need for an AI Ethics Committee in public sector bodies to handle the procurement, development, operations phase of AI systems and to ensure adherence to the Responsible AI principles. Self regulation is recommended for the private sector, with accountability for ethics being assigned to existing leadership. ## Ensuring that AI truly works for All Our recommendations are based on 4 key issues: 1. **Need for concrete overarching principles:** Part 1 of the working document lists certain principles as principles for responsible AI. However, obscurity still persists with regards to the implication of adoption of these principles in the regulatory framework. Thus, we believe that there is a need to spell out both the detailed meaning of the principles which will work as a foundation to build enforcement mechanisms on as well as the implications of adopting them for regulatory frameworks. 2. **Greater role for oversight body:** The draft document proposes the Council for Ethics and Technology as an oversight mechanism, which will be a *“highly participatory advisory body”*. We appreciate the formation of such an oversight body, due to the absence of clear laws and legal requirements governing AI, the Council for Ethics and Technology should have more than mere advisory functions and perform a regulatory assurance role, in conjunction with any forthcoming data authority, under the Personal Data Protection Bill, 2019. This should be specifically with the nature of any AI based system that impacts any legal right of a person. 3. **Robust regulation for the private sector:** The draft document proposes voluntary self-regulation as a starting point for regulating AI in the private sector in India. While self-regulatory efforts are commendable, they should not be made a substitute for laws needed to closely monitor AI. In order to foster a healthy AI ecosystem, soft laws of self-regulation need to be complemented with strict provisions to govern high risk applications of AI. 4. **Risk assessment based restrictions for different sectors**: All AI is not the same since the term envelopes within its multiple technologies. Thus, on the basis of a sector specific risk-assessment study, proportionate restrictions on the use of AI should be in place until an overarching, regulatory framework has been developed. ## Risk Based Assessment - AI in Facial Recognition Technology ### Procedure In our submission, we provided a template risk based assessment for illustrative purposes, which was based on the following procedure: * **Step 1**: The Council must categorize use cases of AI into the 4 types of Algorithmic Systems based on the AI Now Institute’s categorization of algorithmic systems used in their 2018 ‘Algorithmic Accountability Policy Toolkit’. * **Step 2**: The Council must assess the type of data collected by the AI into anonymous data, personal and sensitive personal data,, * **Step 3**: Based on type of data, risk assessment may be done with sensitive personal data having the highest risk assessment and anonymous data the lowest (Scale of 1-3 with 3 being the highest). * **Step 4**: Based on the level of risk assessment combined with the type of algorithmic system, the regulation framework may be designed by the Council. ### The dangers of AI in Facial Recognition Technology Here, we would like to address the use of AI for facial recognition technology (FRT). FRT which collects sensitive personal data for criminal justice purposes should be banned from being developed and deployed in India. Under IFF’s [**Project Panoptic**](https://panoptic.frappe.cloud/), we have been mapping FRT systems across the country which are being developed and deployed without any regulatory framework as well as without any public awareness or transparency pertaining to how they will be used. By our estimation, there are currently 19 different FRT projects which are being used by Police and Security/Intelligence agencies at the Central and State level in different stages of development and deployment. This is being done in the absence of a personal data protection law/regime as well as any specific regulation for FRT. The harms of such use are manifold: * One sort of harm results from the implementation of a faulty FRT system wherein the **technology is inaccurate in identifying and matching faces** from a photo/video to an existing database. Such inaccuracy could lead to a false positive result from the FRT. This may lead to discrimination and strengthening of existing biases. In the present context, a false positive by a FRT system being used by the police could lead to wrongful arrest and detention of an innocent person. * Another type of harm results from the implementation of an accurate FRT system wherein **the technology achieves 100% accuracy in identifying and matching faces** from a photo/video to an existing database. While there have been claims of a fully accurate FRT system, none of these claims have been corroborated by independent review and audit. The National Institute of Standards and Technology (NIST) has extensively tested FRT systems for ‘1:1’ verification and ‘1:many’ identification and how accuracy of these systems vary across demographic groups. These independent studies have concluded that, currently, no FRT system has 100% accuracy. * Probe images for FRT systems are often collected by the police t**hrough CCTV cameras installed in public spaces**. Individuals in a CCTV surveilled area may be aware that they are under surveillance but the assumption is that this surveillance is temporary. Use of CCTV in conjunction with FRT would mean their images will be stored for a longer period of time, if not permanently. This data will also be used to extract particular data points such as the facial features and other biometrics (sensitive personal data) which the individual has not consented to sharing when entering a CCTV surveilled zone and these data points can be used to track future movements of the person. Therefore, integration of FRT with a network of CCTV cameras would make **real time mass surveillance extremely easy**. ### Goes against standards set by the Supreme Court We would like to emphasise that use of FRT for criminal justice purposes goes beyond the standards laid down by the Hon’ble Supreme Court in *Justice K.S. Puttaswamy vs Union of India* (2017 10 SCC 1). The landmark decision lays down certain thresholds which the State must conform to to justify intrusions by the State into people’s right to privacy protected under Article 21 of the Constitution of India. These thresholds are: 1. **Legality**: *Where the intrusion must take place in the presence of a defined regime of law i.e. there must be an anchoring legislation, with a clear set of provisions.* As we know, there is no anchoring legislation in place to regulate the use of FRT by the police. Additionally, we do not have a data protection regime to oversee the collection, processing and storage of data collected by these systems. 2. **Necessity**: *Which justifies that the restriction to people’s privacy (in this case data collection and sharing) is needed in a democratic society to fulfill a legitimate state aim.* Use of FRT by the police has been justified based on reasons pertaining to security of the public and the country itself, with proponents saying it will enable automatic identification and verification through criminal databases. This characterisation is based on the faulty assumption that facial recognition technology is accurate, when ongoing research in the field has shown that completely accurate facial recognition technology has not been developed yet. Use of such inaccurate technology, especially for criminal prosecution, could thus result in a false positive. 3. **Proportionality**: *Where the Government must show among other things that the measure being undertaken has a rational nexus with the objective.* FRT contemplates the collecting of sensitive personal information, intimate information of all individuals in the absence of any reasonable suspicion by collecting images and videos from a scene of crime. This could cast a presumption of criminality on a broad set of people. Collecting sensitive personal information of all individuals who were present at the scene of crime creates a presumption of criminality which is disproportionate to the objective it aims to achieve. 4. **Procedural safeguards**: *Where there is an appropriate independent institutional mechanism, with in-built procedural safeguards aligned with standards of procedure established by law which are just, fair and reasonable to prevent abuse.* In the absence of any checks and balances, function creep becomes an immediate problem wherein the issue of FRT being used for functions more than its stated purpose becomes a reality. Use of FRT without safeguards could result in illegal state-sponsored mass surveillance which would have a chilling effect on fundamental rights such right to freedom of expression, freedom of movement and freedom of association which are guaranteed in the Constitution. ### Recommended Restriction Use of this technology has raised concerns not only in India but also abroad with various civil society organisations such as the Electronic Frontier Foundation, the Algorithmic Justice League and Amnesty International calling for ban on the use of this technology. Calls for a total ban have been gaining momentum due to the fear that use of facial recognition by the police and security/intelligence agencies will not only lead to violation of the rights to privacy and freedom of speech and expression but also lead to human rights violations by helping to increase systemic bias against already marginalised communities. The impact on marginalised communities gains special importance for us locally due to the wide inequality and diversity present in our society. Thus, we recommend that the use of FRT be banned. ### Important Documents 1. NITI Aayog's draft *Working Document: Enforcement Mechanisms for Responsible #AIforAll* ([**link**](https://ourgovdotin.files.wordpress.com/2020/11/niti-working-document-enforcement-mechanisms-for-responsible-aiforall.pdf)) 2. NITI Aayog's *Working Document: Towards Responsible #AIforAll* ([**link**](https://niti.gov.in/sites/default/files/2020-07/Responsible-AI.pdf)) 3. IFF's comments on the draft document ([**link**](https://drive.google.com/file/d/1CKvQ3pSSyDjce952u-qedS30ySkze_CS/view?usp=sharing)) 4. IFF's Project Panoptic FRT Tracker ([**link**](https://panoptic.frappe.cloud/))",
    "light_cleaned_comments": "Man this is really good! Thank you!!"
}